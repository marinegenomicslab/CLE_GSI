@article{Marwick2017,
abstract = {{\textcopyright} 2016 Springer Science+Business Media New YorkThe use of computers and complex software is pervasive in archaeology, yet their role in the analytical pipeline is rarely exposed for other researchers to inspect or reuse. This limits the progress of archaeology because researchers cannot easily reproduce each other's work to verify or extend it. Four general principles of reproducible research that have emerged in other fields are presented. An archaeological case study is described that shows how each principle can be implemented using freely available software. The costs and benefits of implementing reproducible research are assessed. The primary benefit, of sharing data in particular, is increased impact via an increased number of citations. The primary cost is the additional time required to enhance reproducibility, although the exact amount is difficult to quantify.},
author = {Marwick, Ben},
doi = {10.1007/s10816-015-9272-9},
issn = {15737764},
journal = {Journal of Archaeological Method and Theory},
keywords = {Australian archaeology,Computer programming,Open science,Reproducible research,Software engineering},
number = {2},
pages = {424--450},
title = {{Computational Reproducibility in Archaeological Research: Basic Principles and a Case Study of Their Implementation}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2Fs10816-015-9272-9.pdf},
volume = {24},
year = {2017}
}
@article{Gentleman2012,
author = {Gentleman, Robert and Lang, DT},
journal = {Journal of Computational and {\ldots}},
month = {may},
title = {{Statistical analyses and reproducible research}},
url = {https://biostats.bepress.com/bioconductor/paper2 http://amstat.tandfonline.com/doi/abs/10.1198/106186007X178663},
year = {2012}
}
@article{Tajima1989a,
abstract = {The expected number of segregating sites and the expectation of the average number of nucleotide differences among DNA sequences randomly sampled from a population, which is not in equilibrium, have been developed. The results obtained indicate that, in the case where the population size has changed drastically, the number of segregating sites is influenced by the size of the current population more strongly than is the average number of nucleotide differences, while the average number of nucleotide differences is affected by the size of the original population more severely than is the number of segregating sites. The results also indicate that the average number of nucleotide differences is affected by a population bottleneck more strongly than is the number of segregating sites.},
author = {Tajima, F},
isbn = {0016-6731},
issn = {00166731},
journal = {Genetics},
number = {3},
pages = {597--601.},
pmid = {2599369},
title = {{The effect of change in population size on DNA polymorphism}},
volume = {123},
year = {1989}
}
@article{Wang2017,
abstract = {Introduction: D-dimer assay, generally evaluated according to cutoff points calibrated for VTE exclusion, is used to estimate the individual risk of recurrence after a first idiopathic event of venous thromboembolism (VTE). Methods: Commercial D-dimer assays, evaluated according to predetermined cutoff levels for each assay, specific for age (lower in subjects {\textless}70 years) and gender (lower in males), were used in the recent DULCIS study. The present analysis compared the results obtained in the DULCIS with those that might have been had using the following different cutoff criteria: traditional cutoff for VTE exclusion, higher levels in subjects aged ≥60 years, or age multiplied by 10. Results: In young subjects, the DULCIS low cutoff levels resulted in half the recurrent events that would have occurred using the other criteria. In elderly patients, the DULCIS results were similar to those calculated for the two age-adjusted criteria. The adoption of traditional VTE exclusion criteria would have led to positive results in the large majority of elderly subjects, without a significant reduction in the rate of recurrent event. Conclusion: The results confirm the usefulness of the cutoff levels used in DULCIS.},
author = {Wang, Jinliang},
doi = {10.1111/1755-0998.12650},
issn = {17550998},
journal = {Molecular Ecology Resources},
keywords = {admixture,genetic differentiation,genetic structure,markers,population clustering},
month = {sep},
number = {5},
pages = {981--990},
publisher = {Wiley/Blackwell (10.1111)},
title = {{The computer program structure for assigning individuals to populations: easy to use but easier to misuse}},
url = {http://doi.wiley.com/10.1111/1755-0998.12650},
volume = {17},
year = {2017}
}
@article{Puechmaille2016,
abstract = {Inferences of population structure and more precisely the identification of genetically homogeneous groups of indi- viduals are essential to the fields of ecology, evolutionary biology and conservation biology. Such population struc- ture inferences are routinely investigated via the program STRUCTURE implementing a Bayesian algorithm to identify groups of individuals at Hardy–Weinberg and linkage equilibrium. While the method is performing relatively well under various population models with even sampling between subpopulations, the robustness of the method to uneven sample size between subpopulations and/or hierarchical levels of population structure has not yet been tested despite being commonly encountered in empirical data sets. In this study, I used simulated and empirical microsatellite data sets to investigate the impact of uneven sample size between subpopulations and/or hierarchical levels of population structure on the detected population structure. The results demonstrated that uneven sampling often leads to wrong inferences on hierarchical structure and downward-biased estimates of the true number of sub- populations. Distinct subpopulations with reduced sampling tended to be merged together, while at the same time, individuals from extensively sampled subpopulations were generally split, despite belonging to the same panmictic population. Four new supervised methods to detect the number of clusters were developed and tested as part of this study and were found to outperform the existing methods using both evenly and unevenly sampled data sets. Addi- tionally, a subsampling strategy aiming to reduce sampling unevenness between subpopulations is presented and tested. These results altogether demonstrate that when sampling evenness is accounted for, the detection of the correct population structure is greatly improved.},
author = {Puechmaille, Sebastien J.},
doi = {10.1111/1755-0998.12512},
isbn = {1755-0998 (Electronic) 1755-098X (Linking)},
issn = {17550998},
journal = {Molecular Ecology Resources},
keywords = {Clustering algorithm,Genetic differentiation,Hierarchical structure,Sampling scheme,structure software},
month = {may},
number = {3},
pages = {608--627},
pmid = {26856252},
publisher = {Wiley/Blackwell (10.1111)},
title = {{The program structure does not reliably recover the correct population structure when sampling is uneven: Subsampling and new estimators alleviate the problem}},
url = {http://doi.wiley.com/10.1111/1755-0998.12512},
volume = {16},
year = {2016}
}
@misc{Waples2010,
abstract = {Recognition of the importance of cross-validation (‘any technique or instance of assessing how the results of a statistical analysis will generalize to an independent dataset'; Wiktionary, en.wiktionary.org) is one reason that the U.S. Securities and Exchange Commission requires all investment products to carry some variation of the disclaimer, ‘Past performance is no guarantee of future results.' Even a cursory examination of financial behaviour, however, demonstrates that this warning is regularly ignored, even by those who understand what an independent dataset is. In the natural sciences, an analogue to predicting future returns for an investment strategy is predicting power of a particular algorithm to perform with new data. Once again, the key to developing an unbiased assessment of future performance is through testing with independent data—that is, data that were in no way involved in developing the method in the first place. A ‘gold-standard' approach to cross-validation is to divide the data into two parts, one used to develop the algorithm, the other used to test its performance. Because this approach substantially reduces the sample size that can be used in constructing the algorithm, researchers often try other variations of cross-validation to accomplish the same ends. As illustrated by Anderson in this issue of Molecular Ecology Resources, however, not all attempts at cross-validation produce the desired result. Anderson used simulated data to evaluate performance of several software programs designed to identify subsets of loci that can be effective for assigning individuals to population of origin based on multilocus genetic data. Such programs are likely to become increasingly popular as researchers seek ways to streamline routine analyses by focusing on small sets of loci that contain most of the desired signal. Anderson found that although some of the programs made an attempt at cross-validation, all failed to meet the ‘gold standard' of using truly independent data and therefore produced overly optimistic assessments of power of the selected set of loci—a phenomenon known as ‘high grading bias.'},
author = {Waples, Robin S.},
booktitle = {Molecular Ecology},
doi = {10.1111/j.1365-294X.2010.04675.x},
isbn = {1365-294X (Electronic)$\backslash$r0962-1083 (Linking)},
issn = {09621083},
keywords = {assignment tests,cross-validation,discriminant function analysis,independence,jackknife,sample size,split-sample},
month = {jun},
number = {13},
pages = {2599--2601},
pmid = {20636893},
publisher = {Wiley/Blackwell (10.1111)},
title = {{High-grading bias: Subtle problems with assessing power of selected subsets of loci for population assignment}},
url = {http://doi.wiley.com/10.1111/j.1365-294X.2010.04675.x},
volume = {19},
year = {2010}
}
@article{Anderson2010,
abstract = {It is well known that statistical classification procedures should be assessed using data that are separate from those used to train the classifier. This principle is commonly overlooked when the classification procedure in question is population assignment using a set of genetic markers that were chosen specifically on the basis of their allele frequencies from amongst a larger number of candidate markers. This oversight leads to a systematic upward bias in the predicted accuracy of the chosen set of markers for population assignment. Three widely used software programs for selecting markers informative for population assignment suffer from this bias. The extent of this bias is documented through a small set of simulations. The relative effect of the bias is largest when screening many candidate loci from poorly differentiated populations. Simple unbiased methods are presented and their use encouraged.},
author = {Anderson, E. C.},
doi = {10.1111/j.1755-0998.2010.02846.x},
isbn = {1755-0998},
issn = {1755098X},
journal = {Molecular Ecology Resources},
keywords = {Assignment test,Cross-validation,Holdout data,Training data},
month = {mar},
number = {4},
pages = {701--710},
pmid = {21565075},
publisher = {Wiley/Blackwell (10.1111)},
title = {{Assessing the power of informative subsets of loci for population assignment: Standard methods are upwardly biased}},
url = {http://doi.wiley.com/10.1111/j.1755-0998.2010.02846.x},
volume = {10},
year = {2010}
}
@article{Chen2018,
abstract = {1. The use of biomarkers (e.g., genetic, microchemical and morphometric characteristics) to discriminate among and assign individuals to a population can benefit species conservation and management by facilitating our ability to understand population structure and demography. 2. Tools that can evaluate the reliability of large genomic datasets for population discrimination and assignment, as well as allow their integration with non-genetic markers for the same purpose, are lacking. Our R package, assignPOP, provides both functions in a supervised machine-learning framework. 3. assignPOP uses Monte-Carlo and K-fold cross-validation procedures, as well as principal component analysis, to estimate assignment accuracy and membership probabilities, using training (i.e., baseline source population) and test (i.e., validation) datasets that are independent. A user then can build a specified predictive model based on the relative sizes of these datasets and classification functions, including linear discriminant analysis, support vector machine, naive Bayes, decision tree and random forest. 4. assignPOP can benefit any researcher who seeks to use genetic or non-genetic data to infer population structure and membership of individuals. assignPOP is a freely available R package under the GPL license, and can be downloaded from CRAN or at . A comprehensive tutorial can also be found at https://github.com/alexkychen/assignPOP. A comprehensive tutorial can also be found at https://alexkychen.github.io/assignPOP/.},
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0608246v3},
author = {Chen, Kuan Yu and Marschall, Elizabeth A. and Sovic, Michael G. and Fries, Anthony C. and Gibbs, H. Lisle and Ludsin, Stuart A.},
doi = {10.1111/2041-210X.12897},
editor = {Poisot, Timoth{\'{e}}e},
eprint = {0608246v3},
isbn = {4955139574},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {assignment analysis,machine learning,population classification,quantitative genomics},
month = {feb},
number = {2},
pages = {439--446},
pmid = {28199780},
primaryClass = {arXiv:physics},
publisher = {Wiley/Blackwell (10.1111)},
title = {{assignPOP: An r package for population assignment using genetic, non-genetic, or integrated data in a machine-learning framework}},
url = {http://doi.wiley.com/10.1111/2041-210X.12897},
volume = {9},
year = {2018}
}
@article{Gosselin2016,
author = {Gosselin, T and Anderson, Eric C and Bradbury, Ian R},
doi = {doi : 10.5281/zenodo.51453},
journal = {R package},
title = {{assigner: Assignment Analysis with GBS/RAD Data using R.}},
year = {2016}
}
@article{Goudet2005,
abstract = {The package hierfstat for the statistical software r , created by the R Development Core Team, allows the estimate of hierarchical F -statistics from a hierarchy with any numbers of levels. In addition, it allows testing the statistical significance of population differentiation for these different levels, using a generalized likelihood-ratio test. The package hierfstat is available at http://www.unil.ch/popgen /softwares/ hierfstat.htm.},
author = {Goudet, J{\'{e}}r{\^{o}}me},
doi = {10.1111/j.1471-8286.2004.00828.x},
file = {:C$\backslash$:/Users/shannonigans/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goudet - 2005 - HIERFSTAT, a package for R to compute and test hierarchical F-statistics(2).pdf:pdf},
isbn = {1471-8278},
issn = {14718278},
journal = {Molecular Ecology Notes},
keywords = {Population structure,Randomisation tests,Statistics},
month = {mar},
number = {1},
pages = {184--186},
publisher = {Wiley/Blackwell (10.1111)},
title = {{HIERFSTAT, a package for R to compute and test hierarchical F-statistics}},
volume = {5},
year = {2005}
}
@misc{OLeary2018,
abstract = {Sequencing reduced‐representation libraries of restriction site‐associated DNA (RADseq) to identify single nucleotide polymorphisms (SNPs) is quickly becoming a standard methodology for molecular ecologists. Because of the scale of RADseq data sets, putative loci cannot be assessed individually, making the process of filtering noise and correctly identifying biologically meaningful signal more difficult. Artefacts introduced during library preparation and/or bioinformatic processing of SNP data can create patterns that are incorrectly interpreted as indicative of population structure or natural selection. Therefore, it is crucial to carefully consider types of errors that may be introduced during laboratory work and data processing, and how to minimize, detect and remove these errors. Here, we discuss issues inherent to RADseq methodologies that can result in artefacts during library preparation and locus reconstruction resulting in erroneous SNP calls and, ultimately, genotyping error. Further, we describe steps that can be implemented to create a rigorously filtered data set consisting of markers accurately representing independent loci and compare the effect of different combinations of filters on four RAD data sets. At last, we stress the importance of publishing raw sequence data along with final filtered data sets in addition to detailed documentation of filtering steps and quality control measures.},
author = {O'Leary, Shannon J and Puritz, Jonathan B and Willis, Stuart C and Hollenbeck, Christopher M and Portnoy, David S},
booktitle = {Molecular Ecology},
doi = {10.1111/mec.14792},
isbn = {0962-1083},
issn = {1365294X},
keywords = {Conservation genetics,Ecological genetics,Landscape genetics,Molecular evolution,Population ecology,Population genetics-empirical},
month = {aug},
number = {16},
pages = {3193--3206},
publisher = {Wiley/Blackwell (10.1111)},
title = {{These aren't the loci you're looking for: Principles of effective SNP filtering for molecular ecologists}},
volume = {27},
year = {2018}
}
@article{Weir1984,
abstract = {Formulae are given for estimators for the parameters F, $\theta$, f (FIT, FST, FIS) of population structure. As with all such estimators, ratios are used so that their properties are not known exactly, but they have been found to perform satisfactorily in simulations. Unlike the estimators in general use, the formulae do not make assumptions concerning numbers of populations, sample sizes, or heterozygote frequencies. As such, they are suited to small data sets and will aid the comparisons of results of different investigators. A simple weighting procedure is suggested for combining information over alleles and loci, and sample variances may be estimated by a jackknife procedure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Weir, B. S. and Cockerham, C. Clark},
doi = {10.2307/2408641},
eprint = {arXiv:1011.1669v3},
isbn = {00143820},
issn = {00143820},
journal = {Evolution},
month = {nov},
number = {6},
pages = {1358},
pmid = {909345331780073315},
publisher = {Society for the Study of Evolution},
title = {{Estimating F-Statistics for the Analysis of Population Structure}},
url = {http://www.jstor.org/stable/2408641?origin=crossref},
volume = {38},
year = {1984}
}
@article{Garrison2012,
abstract = {The direct detection of haplotypes from short-read DNA sequencing data requires changes to existing small-variant detection methods. Here, we develop a Bayesian statistical framework which is capable of modeling multiallelic loci in sets of individuals with non-uniform copy number. We then describe our implementation of this framework in a haplotype-based variant detector, FreeBayes.},
archivePrefix = {arXiv},
arxivId = {1207.3907},
author = {Garrison, Erik and Marth, Gabor},
doi = {arXiv:1207.3907 [q-bio.GN]},
eprint = {1207.3907},
isbn = {1095-9203 (Electronic)$\backslash$n0036-8075 (Linking)},
issn = {1095-9203},
journal = {Plos One},
number = {3},
pages = {e0151651},
pmid = {24136966},
title = {{Haplotype-based variant detection from short-read sequencing}},
volume = {11},
year = {2012}
}
@article{Danecek2011,
abstract = {SUMMARY: The variant call format (VCF) is a generic format for storing DNA polymorphism data such as SNPs, insertions, deletions and structural variants, together with rich annotations. VCF is usually stored in a compressed manner and can be indexed for fast data retrieval of variants from a range of positions on the reference genome. The format was developed for the 1000 Genomes Project, and has also been adopted by other projects such as UK10K, dbSNP and the NHLBI Exome Project. VCFtools is a software suite that implements various utilities for processing VCF files, including validation, merging, comparing and also provides a general Perl API.$\backslash$n$\backslash$nAVAILABILITY: http://vcftools.sourceforge.net},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Danecek, Petr and Auton, Adam and Abecasis, Goncalo and Albers, Cornelis A. and Banks, Eric and DePristo, Mark A. and Handsaker, Robert E. and Lunter, Gerton and Marth, Gabor T. and Sherry, Stephen T. and McVean, Gilean and Durbin, Richard},
doi = {10.1093/bioinformatics/btr330},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/shannonigans/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Danecek et al. - 2011 - The variant call format and VCFtools(2).pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$n1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {15},
pages = {2156--2158},
pmid = {21653522},
title = {{The variant call format and VCFtools}},
volume = {27},
year = {2011}
}
@article{Li2009,
abstract = {MOTIVATION: The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals.$\backslash$n$\backslash$nRESULTS: We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows-Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is approximately 10-20x faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package.$\backslash$n$\backslash$nAVAILABILITY: http://maq.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1303.3997},
author = {Li, Heng and Durbin, Richard},
doi = {10.1093/bioinformatics/btp324},
eprint = {1303.3997},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {14},
pages = {1754--1760},
pmid = {19451168},
title = {{Fast and accurate short read alignment with Burrows-Wheeler transform}},
volume = {25},
year = {2009}
}
@article{Ilut2014,
abstract = {Next generation sequencing holds great promise for applications of phylogeography, landscape genetics, and population genomics in wild populations of nonmodel species, but the robustness of inferences hinges on careful experimental design and effective bioinformatic removal of predictable artifacts. Addressing this issue, we use published genomes from a tunicate, stickleback, and soybean to illustrate the potential for bioinformatic artifacts and introduce a protocol to minimize two sources of error expected from similarity-based de-novo clustering of stacked reads: the splitting of alleles into different clusters, which creates false homozygosity, and the grouping of paralogs into the same cluster, which creates false heterozygosity. We present an empirical application focused on Ciona savignyi, a tunicate with very high SNP heterozygosity ({\~{}}0.05), because high diversity challenges the computational efficiency of most existing nonmodel pipelines while also potentially exacerbating paralog artifacts. The simulated and empirical data illustrate the advantages of using higher sequence difference clustering thresholds than is typical and demonstrate the utility of our protocol for efficiently identifying an optimum threshold from data without prior knowledge of heterozygosity. The empirical Ciona savignyi data also highlight null alleles as a potentially large source of false homozygosity in restriction-based reduced representation genomic data.},
author = {Ilut, Daniel C. and Nydam, Marie L. and Hare, Matthew P.},
doi = {10.1155/2014/675158},
isbn = {2314-6133},
issn = {23146141},
journal = {BioMed Research International},
pmid = {25057498},
title = {{Defining loci in restriction-based reduced representation genomic data from nonmodel species: Sources of bias and diagnostics for optimal clustering}},
volume = {2014},
year = {2014}
}
@misc{Li2014,
abstract = {Motivation: Whole-genome high-coverage sequencing has been widely used for personal and cancer genomics as well as in various research areas. However, in the lack of an unbiased whole-genome truth set, the global error rate of variant calls and the leading causal artifacts still remain unclear even given the great efforts in the evaluation of variant calling methods. Results: We made ten SNP and INDEL call sets with two read mappers and five variant callers, both on a haploid human genome and a diploid genome at a similar coverage. By investigating false heterozygous calls in the haploid genome, we identified the erroneous realignment in low-complexity regions and the incomplete reference genome with respect to the sample as the two major sources of errors, which press for continued improvements in these two areas. We estimated that the error rate of raw genotype calls is as high as 1 in 10-15kb, but the error rate of post-filtered calls is reduced to 1 in 100-200kb without significant compromise on the sensitivity. Availability: BWA-MEM alignment: http://bit.ly/1g8XqRt; Scripts: https://github.com/lh3/varcmp; Additional data: http://figshare.com/account/projects/1013},
archivePrefix = {arXiv},
arxivId = {1404.0929},
author = {Li, Heng and Wren, Jonathan},
booktitle = {Bioinformatics},
doi = {10.1093/bioinformatics/btu356},
eprint = {1404.0929},
isbn = {1367-4803},
issn = {14602059},
number = {20},
pages = {2843--2851},
pmid = {24974202},
title = {{Toward better understanding of artifacts in variant calling from high-coverage samples}},
volume = {30},
year = {2014}
}
@article{Fu2012,
abstract = {CD-HIT is a widely used program for clustering biological sequences to reduce sequence redundancy and improve the performance of other sequence analyses. In response to the rapid increase in the amount of sequencing data produced by the next-generation sequencing technologies, we have developed a new CD-HIT program accelerated with a novel parallelization strategy and some other techniques to allow efficient clustering of such datasets. Our tests demonstrated very good speedup derived from the parallelization for up to ∼24 cores and a quasi-linear speedup for up to ∼8 cores. The enhanced CD-HIT is capable of handling very large datasets in much shorter time than previous versions.},
author = {Fu, Limin and Niu, Beifang and Zhu, Zhengwei and Wu, Sitao and Li, Weizhong},
doi = {10.1093/bioinformatics/bts565},
isbn = {1367-4811 (Electronic)$\backslash$n1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {23},
pages = {3150--3152},
pmid = {23060610},
title = {{CD-HIT: Accelerated for clustering the next-generation sequencing data}},
volume = {28},
year = {2012}
}
@article{Puritz2014,
abstract = {Restriction-site associated DNA sequencing (RADseq) has become a powerful and useful approach for population genomics. Currently, no software exists that utilizes both paired-end reads from RADseq data to efficiently produce population-informative variant calls, especially for non-model organisms with large effective population sizes and high levels of genetic polymorphism. dDocent is an analysis pipeline with a user-friendly, command-line interface designed to process individually barcoded RADseq data (with double cut sites) into informative SNPs/Indels for population-level analyses. The pipeline, written in BASH, uses data reduction techniques and other stand-alone software packages to perform quality trimming and adapter removal, de novo assembly of RAD loci, read mapping, SNP and Indel calling, and baseline data filtering. Double-digest RAD data from population pairings of three different marine fishes were used to compare dDocent with Stacks, the first generally available, widely used pipeline for analysis of RADseq data. dDocent consistently identified more SNPs shared across greater numbers of individuals and with higher levels of coverage. This is due to the fact that dDocent quality trims instead of filtering, incorporates both forward and reverse reads (including reads with INDEL polymorphisms) in assembly, mapping, and SNP calling. The pipeline and a comprehensive user guide can be found at http://dDocent.wordpress.com.},
archivePrefix = {arXiv},
arxivId = {10.7287/peerj.preprints.270v1},
author = {Puritz, Jonathan B and Hollenbeck, Christopher M and Gold, John R},
doi = {10.7717/peerj.431},
eprint = {peerj.preprints.270v1},
isbn = {6176367670},
issn = {2167-8359},
journal = {PeerJ},
pages = {e431},
pmid = {24949246},
primaryClass = {10.7287},
title = {{dDocent : a RADseq, variant-calling pipeline designed for population genomics of non-model organisms}},
volume = {2},
year = {2014}
}
@article{Chong2012,
abstract = {MOTIVATION: The innovation of restriction-site associated DNA sequencing (RAD-seq) method takes full advantage of next-generation sequencing technology. By clustering paired-end short reads into groups with their own unique tags, RAD-seq assembly problem is divided into subproblems. Fast and accurately clustering and assembling millions of RAD-seq reads with sequencing errors, different levels of heterozygosity and repetitive sequences is a challenging question.$\backslash$n$\backslash$nRESULTS: Rainbow is developed to provide an ultra-fast and memory-efficient solution to clustering and assembling short reads produced by RAD-seq. First, Rainbow clusters reads using a spaced seed method. Then, Rainbow implements a heterozygote calling like strategy to divide potential groups into haplotypes in a top-down manner. And along a guided tree, it iteratively merges sibling leaves in a bottom-up manner if they are similar enough. Here, the similarity is defined by comparing the 2nd reads of a RAD segment. This approach tries to collapse heterozygote while discriminate repetitive sequences. At last, Rainbow uses a greedy algorithm to locally assemble merged reads into contigs. Rainbow not only outputs the optimal but also suboptimal assembly results. Based on simulation and a real guppy RAD-seq data, we show that Rainbow is more competent than the other tools in dealing with RAD-seq data.$\backslash$n$\backslash$nAVAILABILITY: Source code in C, Rainbow is freely available at http://sourceforge.net/projects/bio-rainbow/files/},
author = {Chong, Zechen and Ruan, Jue and Wu, Chung I.},
doi = {10.1093/bioinformatics/bts482},
isbn = {1367-4803},
issn = {13674803},
journal = {Bioinformatics},
number = {21},
pages = {2732--2737},
pmid = {22942077},
title = {{Rainbow: An integrated tool for efficient clustering and assembling RAD-seq reads}},
volume = {28},
year = {2012}
}
@article{Willis2017,
author = {Willis, Stuart C. and Hollenbeck, Christopher M. and Puritz, Jonathan B. and Gold, John R. and Portnoy, David S.},
doi = {10.1111/1755-0998.12647},
issn = {1755098X},
journal = {Molecular Ecology Resources},
month = {feb},
title = {{Haplotyping RAD loci: an efficient method to filter paralogs and account for physical linkage}},
year = {2017}
}
@article{Catchen2013,
abstract = {Massively parallel short-read sequencing technologies, coupled with powerful software platforms, are enabling investigators to analyse tens of thousands of genetic markers. This wealth of data is rapidly expanding and allowing biological questions to be addressed with unprecedented scope and precision. The sizes of the data sets are now posing significant data processing and analysis challenges. Here we describe an extension of the Stacks software package to efficiently use genotype-by-sequencing data for studies of populations of organisms. Stacks now produces core population genomic summary statistics and SNP-by-SNP statistical tests. These statistics can be analysed across a reference genome using a smoothed sliding window. Stacks also now provides several output formats for several commonly used downstream analysis packages. The expanded population genomics functions in Stacks will make it a useful tool to harness the newest generation of massively parallel genotyping data for ecological and evolutionary genetics.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Catchen, Julian and Hohenlohe, Paul A. and Bassham, Susan and Amores, Angel and Cresko, William A.},
doi = {10.1111/mec.12354},
eprint = {NIHMS150003},
isbn = {1365-294X},
issn = {09621083},
journal = {Molecular Ecology},
keywords = {GBS,RAD-seq,genetics,next-generation sequencing,population genomics},
number = {11},
pages = {3124--3140},
pmid = {23701397},
title = {{Stacks: An analysis tool set for population genomics}},
volume = {22},
year = {2013}
}
